{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from rna_resnet import ResNet\n",
    "from keras import callbacks as cb\n",
    "from Calibration_Util import FileIO as io\n",
    "import os\n",
    "import numpy as np\n",
    "import CostFunctions as cf\n",
    "from keras import backend as K\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from plots import heatmap\n",
    "from sys import argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '../data/6pc/'\n",
    "# GTEX as source and TCGA as target\n",
    "source_file = 'unnorm-log-6PC-GTEX-breast-prostate-thyroid.csv'\n",
    "target_file = 'unnorm-log-6PC-TCGA-breast-prostate-thyroid.csv'\n",
    "    \n",
    "source_path = folder + source_file\n",
    "target_path = folder + target_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gtex = source shape = (636, 6)\n",
      "tcga = target shape = (211, 6)\n",
      "setting scales using KNN\n",
      "[12.431486575826753, 24.862973151653506, 49.72594630330701]\n",
      "setting all scale weights to 1\n",
      "Train on 572 samples, validate on 64 samples\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - 1s 874us/step - loss: 1.2328 - val_loss: 1.8634\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 1.2051 - val_loss: 1.8057\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 1.1424 - val_loss: 1.7467\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 1.0384 - val_loss: 1.6481\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.9556 - val_loss: 1.6185\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.9214 - val_loss: 1.5949\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - 0s 237us/step - loss: 0.9489 - val_loss: 1.5110\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.9366 - val_loss: 1.5571\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - 0s 236us/step - loss: 0.9047 - val_loss: 1.5722\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.9131 - val_loss: 1.5232\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - 0s 238us/step - loss: 0.9067 - val_loss: 1.5405\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8877 - val_loss: 1.5133\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8834 - val_loss: 1.4996\n",
      "Epoch 14/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8734 - val_loss: 1.4921\n",
      "Epoch 15/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8641 - val_loss: 1.5501\n",
      "Epoch 16/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8582 - val_loss: 1.4565\n",
      "Epoch 17/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.8785 - val_loss: 1.5509\n",
      "Epoch 18/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8899 - val_loss: 1.5111\n",
      "Epoch 19/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8570 - val_loss: 1.6515\n",
      "Epoch 20/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8881 - val_loss: 1.5046\n",
      "Epoch 21/1000\n",
      "572/572 [==============================] - 0s 239us/step - loss: 0.8668 - val_loss: 1.5005\n",
      "Epoch 22/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.8785 - val_loss: 1.6389\n",
      "Epoch 23/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8802 - val_loss: 1.5475\n",
      "Epoch 24/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8675 - val_loss: 1.5919\n",
      "Epoch 25/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8464 - val_loss: 1.5560\n",
      "Epoch 26/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.8632 - val_loss: 1.6895\n",
      "Epoch 27/1000\n",
      "572/572 [==============================] - 0s 239us/step - loss: 0.8403 - val_loss: 1.5194\n",
      "Epoch 28/1000\n",
      "572/572 [==============================] - 0s 247us/step - loss: 0.8484 - val_loss: 1.5398\n",
      "Epoch 29/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8888 - val_loss: 1.6153\n",
      "Epoch 30/1000\n",
      "572/572 [==============================] - 0s 251us/step - loss: 0.8596 - val_loss: 1.4808\n",
      "Epoch 31/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8759 - val_loss: 1.5777\n",
      "Epoch 32/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8574 - val_loss: 1.6024\n",
      "Epoch 33/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8682 - val_loss: 1.5717\n",
      "Epoch 34/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.8615 - val_loss: 1.5750\n",
      "Epoch 35/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.8527 - val_loss: 1.6040\n",
      "Epoch 36/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8623 - val_loss: 1.4569\n",
      "Epoch 37/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.8386 - val_loss: 1.7451\n",
      "Epoch 38/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8357 - val_loss: 1.5099\n",
      "Epoch 39/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8446 - val_loss: 1.4428\n",
      "Epoch 40/1000\n",
      "572/572 [==============================] - 0s 247us/step - loss: 0.8490 - val_loss: 1.4109\n",
      "Epoch 41/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8396 - val_loss: 1.4908\n",
      "Epoch 42/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8518 - val_loss: 1.4952\n",
      "Epoch 43/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.8549 - val_loss: 1.5709\n",
      "Epoch 44/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8321 - val_loss: 1.4928\n",
      "Epoch 45/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8470 - val_loss: 1.5852\n",
      "Epoch 46/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.8479 - val_loss: 1.4225\n",
      "Epoch 47/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.8481 - val_loss: 1.5579\n",
      "Epoch 48/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.8435 - val_loss: 1.4066\n",
      "Epoch 49/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.8367 - val_loss: 1.4228\n",
      "Epoch 50/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.8364 - val_loss: 1.4068\n",
      "Epoch 51/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8458 - val_loss: 1.4357\n",
      "Epoch 52/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.8496 - val_loss: 1.4062\n",
      "Epoch 53/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.8320 - val_loss: 1.4579\n",
      "Epoch 54/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8559 - val_loss: 1.4321\n",
      "Epoch 55/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.8739 - val_loss: 1.4806\n",
      "Epoch 56/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.8549 - val_loss: 1.4174\n",
      "Epoch 57/1000\n",
      "572/572 [==============================] - 0s 249us/step - loss: 0.8488 - val_loss: 1.4313\n",
      "Epoch 58/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8539 - val_loss: 1.4817\n",
      "Epoch 59/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8312 - val_loss: 1.3707\n",
      "Epoch 60/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8235 - val_loss: 1.4889\n",
      "Epoch 61/1000\n",
      "572/572 [==============================] - 0s 239us/step - loss: 0.8429 - val_loss: 1.5789\n",
      "Epoch 62/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8542 - val_loss: 1.3868\n",
      "Epoch 63/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8518 - val_loss: 1.3868\n",
      "Epoch 64/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8444 - val_loss: 1.6591\n",
      "Epoch 65/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.8384 - val_loss: 1.6439\n",
      "Epoch 66/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8451 - val_loss: 1.4229\n",
      "Epoch 67/1000\n",
      "572/572 [==============================] - 0s 247us/step - loss: 0.8449 - val_loss: 1.6519\n",
      "Epoch 68/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8493 - val_loss: 1.7015\n",
      "Epoch 69/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8436 - val_loss: 1.5569\n",
      "Epoch 70/1000\n",
      "572/572 [==============================] - 0s 247us/step - loss: 0.8240 - val_loss: 1.4946\n",
      "Epoch 71/1000\n",
      "572/572 [==============================] - 0s 250us/step - loss: 0.8187 - val_loss: 1.4354\n",
      "Epoch 72/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8087 - val_loss: 1.4430\n",
      "Epoch 73/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8210 - val_loss: 1.4752\n",
      "Epoch 74/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8453 - val_loss: 1.6027\n",
      "Epoch 75/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8488 - val_loss: 1.5714\n",
      "Epoch 76/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8345 - val_loss: 1.4230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.8322 - val_loss: 1.4716\n",
      "Epoch 78/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.8181 - val_loss: 1.5047\n",
      "Epoch 79/1000\n",
      "572/572 [==============================] - 0s 249us/step - loss: 0.8368 - val_loss: 1.5287\n",
      "Epoch 80/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8352 - val_loss: 1.5170\n",
      "Epoch 81/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8372 - val_loss: 1.5681\n",
      "Epoch 82/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.8314 - val_loss: 1.3395\n",
      "Epoch 83/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8436 - val_loss: 1.4369\n",
      "Epoch 84/1000\n",
      "572/572 [==============================] - 0s 248us/step - loss: 0.8171 - val_loss: 1.5140\n",
      "Epoch 85/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.8432 - val_loss: 1.5214\n",
      "Epoch 86/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.8435 - val_loss: 1.4649\n",
      "Epoch 87/1000\n",
      "572/572 [==============================] - 0s 247us/step - loss: 0.8224 - val_loss: 1.6061\n",
      "Epoch 88/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8457 - val_loss: 1.6275\n",
      "Epoch 89/1000\n",
      "572/572 [==============================] - 0s 248us/step - loss: 0.8465 - val_loss: 1.3622\n",
      "Epoch 90/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8472 - val_loss: 1.5076\n",
      "Epoch 91/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8271 - val_loss: 1.6057\n",
      "Epoch 92/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.8364 - val_loss: 1.3987\n",
      "Epoch 93/1000\n",
      "572/572 [==============================] - 0s 248us/step - loss: 0.8361 - val_loss: 1.4405\n",
      "Epoch 94/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8267 - val_loss: 1.3712\n",
      "Epoch 95/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8218 - val_loss: 1.3621\n",
      "Epoch 96/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8380 - val_loss: 1.3941\n",
      "Epoch 97/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.8257 - val_loss: 1.5131\n",
      "Epoch 98/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8111 - val_loss: 1.5043\n",
      "Epoch 99/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8233 - val_loss: 1.4666\n",
      "Epoch 100/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8189 - val_loss: 1.5863\n",
      "Epoch 101/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8252 - val_loss: 1.3617\n",
      "Epoch 102/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.8358 - val_loss: 1.4280\n",
      "Epoch 103/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8231 - val_loss: 1.4064\n",
      "Epoch 104/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8503 - val_loss: 1.3970\n",
      "Epoch 105/1000\n",
      "572/572 [==============================] - 0s 247us/step - loss: 0.8734 - val_loss: 1.5085\n",
      "Epoch 106/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.9149 - val_loss: 1.5484\n",
      "Epoch 107/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.8455 - val_loss: 1.4945\n",
      "Epoch 108/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8348 - val_loss: 1.4703\n",
      "Epoch 109/1000\n",
      "572/572 [==============================] - 0s 249us/step - loss: 0.8191 - val_loss: 1.4779\n",
      "Epoch 110/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8183 - val_loss: 1.4642\n",
      "Epoch 111/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8458 - val_loss: 1.7079\n",
      "Epoch 112/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.8338 - val_loss: 1.5988\n",
      "Epoch 113/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8469 - val_loss: 1.4705\n",
      "Epoch 114/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.8258 - val_loss: 1.4215\n",
      "Epoch 115/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8567 - val_loss: 1.3806\n",
      "Epoch 116/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8450 - val_loss: 1.4533\n",
      "Epoch 117/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.8384 - val_loss: 1.5205\n",
      "Epoch 118/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8258 - val_loss: 1.4229\n",
      "Epoch 119/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.8272 - val_loss: 1.4345\n",
      "Epoch 120/1000\n",
      "572/572 [==============================] - 0s 238us/step - loss: 0.8283 - val_loss: 1.4839\n",
      "Epoch 121/1000\n",
      "572/572 [==============================] - 0s 254us/step - loss: 0.8178 - val_loss: 1.3856\n",
      "Epoch 122/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8104 - val_loss: 1.3521\n",
      "Epoch 123/1000\n",
      "572/572 [==============================] - 0s 247us/step - loss: 0.8207 - val_loss: 1.3991\n",
      "Epoch 124/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8268 - val_loss: 1.3622\n",
      "Epoch 125/1000\n",
      "572/572 [==============================] - 0s 254us/step - loss: 0.8266 - val_loss: 1.6096\n",
      "Epoch 126/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8300 - val_loss: 1.3907\n",
      "Epoch 127/1000\n",
      "572/572 [==============================] - 0s 247us/step - loss: 0.8141 - val_loss: 1.4369\n",
      "Epoch 128/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8284 - val_loss: 1.3558\n",
      "Epoch 129/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8215 - val_loss: 1.3841\n",
      "Epoch 130/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8218 - val_loss: 1.4083\n",
      "Epoch 131/1000\n",
      "572/572 [==============================] - 0s 239us/step - loss: 0.8294 - val_loss: 1.4142\n",
      "Epoch 132/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.8341 - val_loss: 1.4909\n",
      "Epoch 133/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8232 - val_loss: 1.4341\n",
      "Epoch 134/1000\n",
      "572/572 [==============================] - 0s 237us/step - loss: 0.8202 - val_loss: 1.4022\n",
      "Epoch 135/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8184 - val_loss: 1.5890\n",
      "Epoch 136/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8291 - val_loss: 1.4120\n",
      "Epoch 137/1000\n",
      "572/572 [==============================] - 0s 247us/step - loss: 0.8305 - val_loss: 1.5305\n",
      "Epoch 138/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8305 - val_loss: 1.3729\n",
      "Epoch 139/1000\n",
      "572/572 [==============================] - 0s 238us/step - loss: 0.8145 - val_loss: 1.4060\n",
      "Epoch 140/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8067 - val_loss: 1.6584\n",
      "Epoch 141/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.8410 - val_loss: 1.3514\n",
      "Epoch 142/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8238 - val_loss: 1.4430\n",
      "Epoch 143/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8269 - val_loss: 1.4704\n",
      "Epoch 144/1000\n",
      "572/572 [==============================] - 0s 252us/step - loss: 0.8283 - val_loss: 1.4517\n",
      "Epoch 145/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.8089 - val_loss: 1.5019\n",
      "Epoch 146/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8049 - val_loss: 1.5063\n",
      "Epoch 147/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.8261 - val_loss: 1.6145\n",
      "Epoch 148/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8048 - val_loss: 1.3540\n",
      "Epoch 149/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.8375 - val_loss: 1.4868\n",
      "Epoch 150/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.8319 - val_loss: 1.6307\n",
      "Epoch 151/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8196 - val_loss: 1.6645\n",
      "Epoch 152/1000\n",
      "572/572 [==============================] - 0s 247us/step - loss: 0.8239 - val_loss: 1.3710\n",
      "Epoch 153/1000\n",
      "572/572 [==============================] - 0s 238us/step - loss: 0.8246 - val_loss: 1.3861\n",
      "Epoch 154/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572/572 [==============================] - 0s 246us/step - loss: 0.8044 - val_loss: 1.4321\n",
      "Epoch 155/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8360 - val_loss: 1.5092\n",
      "Epoch 156/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8314 - val_loss: 1.5004\n",
      "Epoch 157/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8060 - val_loss: 1.3636\n",
      "Epoch 158/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8012 - val_loss: 1.5028\n",
      "Epoch 159/1000\n",
      "572/572 [==============================] - 0s 239us/step - loss: 0.8110 - val_loss: 1.6082\n",
      "Epoch 160/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.8195 - val_loss: 1.4616\n",
      "Epoch 161/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8052 - val_loss: 1.3581\n",
      "Epoch 162/1000\n",
      "572/572 [==============================] - 0s 239us/step - loss: 0.8524 - val_loss: 1.3465\n",
      "Epoch 163/1000\n",
      "572/572 [==============================] - 0s 238us/step - loss: 0.8400 - val_loss: 1.3801\n",
      "Epoch 164/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8376 - val_loss: 1.4901\n",
      "Epoch 165/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8184 - val_loss: 1.3918\n",
      "Epoch 166/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.8230 - val_loss: 1.4500\n",
      "Epoch 167/1000\n",
      "572/572 [==============================] - 0s 237us/step - loss: 0.8216 - val_loss: 1.4927\n",
      "Epoch 168/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.8391 - val_loss: 1.6381\n",
      "Epoch 169/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8357 - val_loss: 1.5087\n",
      "Epoch 170/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8325 - val_loss: 1.5267\n",
      "Epoch 171/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.8048 - val_loss: 1.7121\n",
      "Epoch 172/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7977 - val_loss: 1.5627\n",
      "Epoch 173/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.8031 - val_loss: 1.6470\n",
      "Epoch 174/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.8008 - val_loss: 1.6591\n",
      "Epoch 175/1000\n",
      "572/572 [==============================] - 0s 249us/step - loss: 0.8049 - val_loss: 1.5843\n",
      "Epoch 176/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8173 - val_loss: 1.7175\n",
      "Epoch 177/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.8168 - val_loss: 1.4252\n",
      "Epoch 178/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8188 - val_loss: 1.4485\n",
      "Epoch 179/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8134 - val_loss: 1.6511\n",
      "Epoch 180/1000\n",
      "572/572 [==============================] - 0s 235us/step - loss: 0.8198 - val_loss: 1.4791\n",
      "Epoch 181/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8100 - val_loss: 1.8576\n",
      "Epoch 182/1000\n",
      "572/572 [==============================] - 0s 248us/step - loss: 0.8162 - val_loss: 1.3628\n",
      "finished training\n",
      "Running sanity check...\n",
      "predicting on self.source\n",
      "setting scales using KNN\n",
      "[9.045918810281686, 18.09183762056337, 36.18367524112674]\n",
      "setting all scale weights to 1\n",
      "MMD before: 0.8591704369\n",
      "MMD after: 0.4947493672\n"
     ]
    }
   ],
   "source": [
    "rnaNet0 = ResNet(layer_sizes=[6, 20])\n",
    "rnaNet0.load_data(source_path=source_path, target_path=target_path)\n",
    "print(\"gtex = source shape = \" + str(rnaNet0.source.shape))\n",
    "print(\"tcga = target shape = \" + str(rnaNet0.target.shape))\n",
    "rnaNet0.init_res_net()\n",
    "callbacks=[rnaNet0.lrate, cb.EarlyStopping(monitor='val_loss', patience=100, mode='auto')]\n",
    "rnaNet0.train(epochs=1000, callbacks=callbacks, batch_size=50)\n",
    "print(\"finished training\")\n",
    "\n",
    "print(\"Running sanity check...\")\n",
    "rnaNet0.predict()\n",
    "source = rnaNet0.source.astype('float32')\n",
    "target = rnaNet0.target.astype('float32')\n",
    "calibrated_source = rnaNet0.calibrated_source.astype('float32')\n",
    "\n",
    "mmd = cf.MMD(source, target, MMDTargetSampleSize=target.shape[0], n_neighbors=10)\n",
    "mmd_before = K.eval(mmd.cost(source, target))\n",
    "mmd_after = K.eval(mmd.cost(calibrated_source, target))\n",
    "\n",
    "print(\"MMD before: %0.10f\" % mmd_before)\n",
    "print(\"MMD after: %0.10f\" % mmd_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gtex = source shape = (636, 6)\n",
      "tcga = target shape = (211, 6)\n",
      "setting scales using KNN\n",
      "[12.253831663266027, 24.507663326532054, 49.01532665306411]\n",
      "setting all scale weights to 1\n",
      "Train on 572 samples, validate on 64 samples\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - 0s 720us/step - loss: 1.1704 - val_loss: 1.8165\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 1.1389 - val_loss: 1.7456\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - 0s 248us/step - loss: 1.0941 - val_loss: 1.8538\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - 0s 251us/step - loss: 0.9994 - val_loss: 1.8929\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.9225 - val_loss: 1.8890\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8833 - val_loss: 1.9174\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - 0s 247us/step - loss: 0.8608 - val_loss: 1.8778\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8560 - val_loss: 1.8993\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8341 - val_loss: 1.7991\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8249 - val_loss: 1.8377\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.8508 - val_loss: 1.7554\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - 0s 252us/step - loss: 0.8242 - val_loss: 1.8257\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.8394 - val_loss: 1.8528\n",
      "Epoch 14/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8222 - val_loss: 1.8542\n",
      "Epoch 15/1000\n",
      "572/572 [==============================] - 0s 238us/step - loss: 0.8199 - val_loss: 1.8104\n",
      "Epoch 16/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.8021 - val_loss: 1.6985\n",
      "Epoch 17/1000\n",
      "572/572 [==============================] - 0s 247us/step - loss: 0.8032 - val_loss: 1.6456\n",
      "Epoch 18/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8084 - val_loss: 1.4193\n",
      "Epoch 19/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8070 - val_loss: 1.5582\n",
      "Epoch 20/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7916 - val_loss: 1.5937\n",
      "Epoch 21/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8264 - val_loss: 1.5930\n",
      "Epoch 22/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8097 - val_loss: 1.5930\n",
      "Epoch 23/1000\n",
      "572/572 [==============================] - 0s 251us/step - loss: 0.8178 - val_loss: 1.5891\n",
      "Epoch 24/1000\n",
      "572/572 [==============================] - 0s 249us/step - loss: 0.7987 - val_loss: 1.5218\n",
      "Epoch 25/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.7931 - val_loss: 1.5986\n",
      "Epoch 26/1000\n",
      "572/572 [==============================] - 0s 247us/step - loss: 0.8298 - val_loss: 1.6150\n",
      "Epoch 27/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.7995 - val_loss: 1.7010\n",
      "Epoch 28/1000\n",
      "572/572 [==============================] - 0s 247us/step - loss: 0.7930 - val_loss: 1.6751\n",
      "Epoch 29/1000\n",
      "572/572 [==============================] - 0s 248us/step - loss: 0.8156 - val_loss: 1.4194\n",
      "Epoch 30/1000\n",
      "572/572 [==============================] - 0s 247us/step - loss: 0.8048 - val_loss: 1.5452\n",
      "Epoch 31/1000\n",
      "572/572 [==============================] - 0s 251us/step - loss: 0.7921 - val_loss: 1.8420\n",
      "Epoch 32/1000\n",
      "572/572 [==============================] - 0s 248us/step - loss: 0.8235 - val_loss: 1.7089\n",
      "Epoch 33/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.8015 - val_loss: 1.6624\n",
      "Epoch 34/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.7882 - val_loss: 1.6453\n",
      "Epoch 35/1000\n",
      "572/572 [==============================] - 0s 248us/step - loss: 0.8199 - val_loss: 1.8114\n",
      "Epoch 36/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.7880 - val_loss: 1.3720\n",
      "Epoch 37/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.7749 - val_loss: 1.5754\n",
      "Epoch 38/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.7961 - val_loss: 1.7596\n",
      "Epoch 39/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.7897 - val_loss: 1.6345\n",
      "Epoch 40/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.7710 - val_loss: 1.5410\n",
      "Epoch 41/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.7845 - val_loss: 1.5317\n",
      "Epoch 42/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.8028 - val_loss: 1.6607\n",
      "Epoch 43/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.7903 - val_loss: 1.6143\n",
      "Epoch 44/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.8006 - val_loss: 1.6345\n",
      "Epoch 45/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7910 - val_loss: 1.4044\n",
      "Epoch 46/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.7745 - val_loss: 1.4451\n",
      "Epoch 47/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.7676 - val_loss: 1.4037\n",
      "Epoch 48/1000\n",
      "572/572 [==============================] - 0s 238us/step - loss: 0.8168 - val_loss: 1.4740\n",
      "Epoch 49/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.8152 - val_loss: 1.6268\n",
      "Epoch 50/1000\n",
      "572/572 [==============================] - 0s 238us/step - loss: 0.7788 - val_loss: 1.7043\n",
      "Epoch 51/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.7668 - val_loss: 1.7116\n",
      "Epoch 52/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.7974 - val_loss: 1.5875\n",
      "Epoch 53/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7832 - val_loss: 1.5928\n",
      "Epoch 54/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.7975 - val_loss: 1.5901\n",
      "Epoch 55/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.7661 - val_loss: 1.3418\n",
      "Epoch 56/1000\n",
      "572/572 [==============================] - 0s 239us/step - loss: 0.7601 - val_loss: 1.2792\n",
      "Epoch 57/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7768 - val_loss: 1.6317\n",
      "Epoch 58/1000\n",
      "572/572 [==============================] - 0s 239us/step - loss: 0.7671 - val_loss: 1.5804\n",
      "Epoch 59/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.7636 - val_loss: 1.4981\n",
      "Epoch 60/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.7684 - val_loss: 1.3066\n",
      "Epoch 61/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.7437 - val_loss: 1.6412\n",
      "Epoch 62/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7722 - val_loss: 1.4066\n",
      "Epoch 63/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.7535 - val_loss: 1.4590\n",
      "Epoch 64/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.7659 - val_loss: 1.3973\n",
      "Epoch 65/1000\n",
      "572/572 [==============================] - 0s 238us/step - loss: 0.7814 - val_loss: 1.7638\n",
      "Epoch 66/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.7545 - val_loss: 1.4124\n",
      "Epoch 67/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.7621 - val_loss: 1.3731\n",
      "Epoch 68/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.7783 - val_loss: 1.4623\n",
      "Epoch 69/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.7681 - val_loss: 1.2793\n",
      "Epoch 70/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7541 - val_loss: 1.4795\n",
      "Epoch 71/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.8039 - val_loss: 1.6250\n",
      "Epoch 72/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.7717 - val_loss: 1.5804\n",
      "Epoch 73/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7704 - val_loss: 1.3999\n",
      "Epoch 74/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7844 - val_loss: 1.4222\n",
      "Epoch 75/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7714 - val_loss: 1.7794\n",
      "Epoch 76/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.7585 - val_loss: 1.5881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.7758 - val_loss: 1.4167\n",
      "Epoch 78/1000\n",
      "572/572 [==============================] - 0s 238us/step - loss: 0.7803 - val_loss: 1.5175\n",
      "Epoch 79/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.7480 - val_loss: 1.8275\n",
      "Epoch 80/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.7167 - val_loss: 1.6795\n",
      "Epoch 81/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.7475 - val_loss: 1.5276\n",
      "Epoch 82/1000\n",
      "572/572 [==============================] - 0s 239us/step - loss: 0.7817 - val_loss: 1.6484\n",
      "Epoch 83/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7453 - val_loss: 1.3726\n",
      "Epoch 84/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7666 - val_loss: 1.5652\n",
      "Epoch 85/1000\n",
      "572/572 [==============================] - 0s 239us/step - loss: 0.7616 - val_loss: 1.8567\n",
      "Epoch 86/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.7560 - val_loss: 1.7372\n",
      "Epoch 87/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.7576 - val_loss: 1.8062\n",
      "Epoch 88/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.7712 - val_loss: 1.6574\n",
      "Epoch 89/1000\n",
      "572/572 [==============================] - 0s 247us/step - loss: 0.7846 - val_loss: 1.8113\n",
      "Epoch 90/1000\n",
      "572/572 [==============================] - 0s 236us/step - loss: 0.7595 - val_loss: 1.8560\n",
      "Epoch 91/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.7504 - val_loss: 1.6826\n",
      "Epoch 92/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.7564 - val_loss: 1.3733\n",
      "Epoch 93/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.7694 - val_loss: 1.2615\n",
      "Epoch 94/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7462 - val_loss: 1.2527\n",
      "Epoch 95/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.7533 - val_loss: 1.5331\n",
      "Epoch 96/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.7303 - val_loss: 1.5662\n",
      "Epoch 97/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.7563 - val_loss: 1.5137\n",
      "Epoch 98/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.7582 - val_loss: 1.5684\n",
      "Epoch 99/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.7476 - val_loss: 1.5137\n",
      "Epoch 100/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7616 - val_loss: 1.8797\n",
      "Epoch 101/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7532 - val_loss: 1.5856\n",
      "Epoch 102/1000\n",
      "572/572 [==============================] - 0s 238us/step - loss: 0.7505 - val_loss: 1.5314\n",
      "Epoch 103/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.7909 - val_loss: 1.6451\n",
      "Epoch 104/1000\n",
      "572/572 [==============================] - 0s 239us/step - loss: 0.7572 - val_loss: 1.5141\n",
      "Epoch 105/1000\n",
      "572/572 [==============================] - 0s 239us/step - loss: 0.7485 - val_loss: 1.6650\n",
      "Epoch 106/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7483 - val_loss: 1.6285\n",
      "Epoch 107/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.7807 - val_loss: 1.8867\n",
      "Epoch 108/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.7554 - val_loss: 1.7397\n",
      "Epoch 109/1000\n",
      "572/572 [==============================] - 0s 246us/step - loss: 0.7936 - val_loss: 1.2598\n",
      "Epoch 110/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.7293 - val_loss: 1.4925\n",
      "Epoch 111/1000\n",
      "572/572 [==============================] - 0s 239us/step - loss: 0.7631 - val_loss: 1.6296\n",
      "Epoch 112/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7460 - val_loss: 1.4805\n",
      "Epoch 113/1000\n",
      "572/572 [==============================] - 0s 240us/step - loss: 0.7528 - val_loss: 1.4650\n",
      "Epoch 114/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.7461 - val_loss: 1.5243\n",
      "Epoch 115/1000\n",
      "572/572 [==============================] - 0s 247us/step - loss: 0.7556 - val_loss: 1.7174\n",
      "Epoch 116/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.7498 - val_loss: 1.8306\n",
      "Epoch 117/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7519 - val_loss: 1.5225\n",
      "Epoch 118/1000\n",
      "572/572 [==============================] - 0s 245us/step - loss: 0.7212 - val_loss: 1.6152\n",
      "Epoch 119/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.7654 - val_loss: 1.5713\n",
      "Epoch 120/1000\n",
      "572/572 [==============================] - 0s 235us/step - loss: 0.7372 - val_loss: 1.5574\n",
      "Epoch 121/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7339 - val_loss: 1.8266\n",
      "Epoch 122/1000\n",
      "572/572 [==============================] - 0s 243us/step - loss: 0.7430 - val_loss: 1.5423\n",
      "Epoch 123/1000\n",
      "572/572 [==============================] - 0s 237us/step - loss: 0.7478 - val_loss: 1.2534\n",
      "Epoch 124/1000\n",
      "572/572 [==============================] - 0s 238us/step - loss: 0.7371 - val_loss: 1.5218\n",
      "Epoch 125/1000\n",
      "572/572 [==============================] - 0s 237us/step - loss: 0.7828 - val_loss: 1.4395\n",
      "Epoch 126/1000\n",
      "572/572 [==============================] - 0s 238us/step - loss: 0.7559 - val_loss: 1.6378\n",
      "Epoch 127/1000\n",
      "572/572 [==============================] - 0s 244us/step - loss: 0.7582 - val_loss: 1.7891\n",
      "Epoch 128/1000\n",
      "572/572 [==============================] - 0s 241us/step - loss: 0.7357 - val_loss: 1.5843\n",
      "Epoch 129/1000\n",
      "572/572 [==============================] - 0s 239us/step - loss: 0.7408 - val_loss: 1.4970\n",
      "Epoch 130/1000\n",
      "572/572 [==============================] - 0s 242us/step - loss: 0.7608 - val_loss: 1.6552\n",
      "Epoch 131/1000\n",
      "550/572 [===========================>..] - ETA: 0s - loss: 0.7651"
     ]
    }
   ],
   "source": [
    "rnaNet1 = ResNet(layer_sizes=[6, 20])\n",
    "rnaNet1.load_data(source_path=source_path, target_path=target_path)\n",
    "print(\"gtex = source shape = \" + str(rnaNet1.source.shape))\n",
    "print(\"tcga = target shape = \" + str(rnaNet1.target.shape))\n",
    "rnaNet1.init_res_net()\n",
    "callbacks=[rnaNet1.lrate, cb.EarlyStopping(monitor='val_loss', patience=100, mode='auto')]\n",
    "rnaNet1.train(epochs=1000, callbacks=callbacks, batch_size=50)\n",
    "print(\"finished training\")\n",
    "\n",
    "print(\"Running sanity check...\")\n",
    "rnaNet1.predict()\n",
    "source = rnaNet1.source.astype('float32')\n",
    "target = rnaNet1.target.astype('float32')\n",
    "calibrated_source = rnaNet1.calibrated_source.astype('float32')\n",
    "\n",
    "mmd = cf.MMD(source, target, MMDTargetSampleSize=target.shape[0], n_neighbors=10)\n",
    "mmd_before = K.eval(mmd.cost(source, target))\n",
    "mmd_after = K.eval(mmd.cost(calibrated_source, target))\n",
    "\n",
    "print(\"MMD before: %0.10f\" % mmd_before)\n",
    "print(\"MMD after: %0.10f\" % mmd_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnaNet0.predict()\n",
    "source = rnaNet0.source.astype('float32')\n",
    "target = rnaNet0.target.astype('float32')\n",
    "calibrated_source = rnaNet0.calibrated_source.astype('float32')\n",
    "\n",
    "mmd = cf.MMD(source, target, MMDTargetSampleSize=target.shape[0], n_neighbors=10)\n",
    "mmd_before = K.eval(mmd.cost(source, target))\n",
    "mmd_after = K.eval(mmd.cost(calibrated_source, target))\n",
    "\n",
    "print(\"MMD before: %0.10f\" % mmd_before)\n",
    "print(\"MMD after: %0.10f\" % mmd_after)\n",
    "\n",
    "source = rnaNet1.source.astype('float32')\n",
    "target = rnaNet1.target.astype('float32')\n",
    "calibrated_source = rnaNet1.calibrated_source.astype('float32')\n",
    "\n",
    "mmd = cf.MMD(source, target, MMDTargetSampleSize=target.shape[0], n_neighbors=10)\n",
    "mmd_before = K.eval(mmd.cost(source, target))\n",
    "mmd_after = K.eval(mmd.cost(calibrated_source, target))\n",
    "\n",
    "print(\"MMD before: %0.10f\" % mmd_before)\n",
    "print(\"MMD after: %0.10f\" % mmd_after)\n",
    "\n",
    "rnaNet0.predict(data=source)\n",
    "rnaNet1.predict(rnaNet0.calibrated_source)\n",
    "calibrated_source = rnaNet1.calibrated_source\n",
    "\n",
    "mmd = cf.MMD(source, target, MMDTargetSampleSize=target.shape[0], n_neighbors=10)\n",
    "mmd_before = K.eval(mmd.cost(source, target))\n",
    "mmd_after = K.eval(mmd.cost(calibrated_source, target))\n",
    "\n",
    "print(\"MMD before: %0.10f\" % mmd_before)\n",
    "print(\"MMD after: %0.10f\" % mmd_after)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
