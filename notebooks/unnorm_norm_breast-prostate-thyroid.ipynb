{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unnormalized norm (z-score): breast, prostate, thyroid\n",
    "\n",
    "The unnormalized and z-scored breast, prostate and thyroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ggplot/utils.py:81: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.\n",
      "You can access Timestamp as pandas.Timestamp\n",
      "  pd.tslib.Timestamp,\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ggplot/stats/smoothers.py:4: FutureWarning: The pandas.lib module is deprecated and will be removed in a future version. These are private functions and can be accessed from pandas._libs.lib instead\n",
      "  from pandas.lib import Timestamp\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "from train_RNA_ResNet import ResNet\n",
    "from keras import callbacks as cb\n",
    "from Calibration_Util import FileIO as io\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnaNet = ResNet()\n",
    "\n",
    "# GTEX as source and TCGA as target\n",
    "source_file = 'unnorm-norm-20PC-GTEX-breast-prostate-thyroid.csv'\n",
    "target_file = 'unnorm-norm-20PC-TCGA-breast-prostate-thyroid.csv'\n",
    "source_path = os.path.join(io.DeepLearningRoot(), 'data/unnorm/breast-prostate-thyroid/' + source_file)\n",
    "target_path = os.path.join(io.DeepLearningRoot(), 'data/unnorm/breast-prostate-thyroid/' + target_file)\n",
    "\n",
    "# Make GTEX target and TCGA source\n",
    "# target_file = 'unnorm-log-20PC-GTEX-breast-prostate-thyroid.csv'\n",
    "# source_file = 'unnorm-log-20PC-TCGA-breast-prostate-thyroid.csv'\n",
    "# source_path = os.path.join(io.DeepLearningRoot(), 'data/unnorm/' + source_file)\n",
    "# target_path = os.path.join(io.DeepLearningRoot(), 'data/unnorm/' + target_file)\n",
    "\n",
    "\n",
    "rnaNet.load_data(source_path=source_path,\n",
    "                target_path=target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gtex = source shape = (636, 20)\n",
      "tcga = target shape = (211, 20)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\ngtex = source shape = \" + str(rnaNet.source.shape))\n",
    "print(\"tcga = target shape = \" + str(rnaNet.target.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting scales using KNN\n",
      "[28.098399017358574, 56.196798034717148, 112.3935960694343]\n",
      "setting all scale weights to 1\n"
     ]
    }
   ],
   "source": [
    "rnaNet.init_res_net(cost='MMD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 572 samples, validate on 64 samples\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - 1s 1ms/step - loss: 1.3060 - val_loss: 1.8707\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 1.1142 - val_loss: 1.7265\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - 0s 493us/step - loss: 1.0324 - val_loss: 1.6531\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - 0s 489us/step - loss: 1.0003 - val_loss: 1.5702\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - 0s 494us/step - loss: 0.9943 - val_loss: 1.5555\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 0.9816 - val_loss: 1.5659\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - 0s 485us/step - loss: 0.9640 - val_loss: 1.5027\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - 0s 490us/step - loss: 0.9488 - val_loss: 1.4971\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - 0s 488us/step - loss: 0.9535 - val_loss: 1.5528\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - 0s 488us/step - loss: 0.9282 - val_loss: 1.5941\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - 0s 489us/step - loss: 0.9432 - val_loss: 1.6868\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - 0s 491us/step - loss: 0.9237 - val_loss: 1.6386\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - 0s 492us/step - loss: 0.9327 - val_loss: 1.5062\n",
      "Epoch 14/1000\n",
      "572/572 [==============================] - 0s 496us/step - loss: 0.9286 - val_loss: 1.5756\n",
      "Epoch 15/1000\n",
      "572/572 [==============================] - 0s 490us/step - loss: 0.9333 - val_loss: 1.5761\n",
      "Epoch 16/1000\n",
      "572/572 [==============================] - 0s 488us/step - loss: 0.9332 - val_loss: 1.6057\n",
      "Epoch 17/1000\n",
      "572/572 [==============================] - 0s 489us/step - loss: 0.9659 - val_loss: 1.5823\n",
      "Epoch 18/1000\n",
      "572/572 [==============================] - 0s 495us/step - loss: 0.9324 - val_loss: 1.5742\n",
      "Epoch 19/1000\n",
      "572/572 [==============================] - 0s 485us/step - loss: 0.9195 - val_loss: 1.5851\n",
      "Epoch 20/1000\n",
      "572/572 [==============================] - 0s 488us/step - loss: 0.9240 - val_loss: 1.4630\n",
      "Epoch 21/1000\n",
      "572/572 [==============================] - 0s 490us/step - loss: 0.9247 - val_loss: 1.6137\n",
      "Epoch 22/1000\n",
      "572/572 [==============================] - 0s 493us/step - loss: 0.9279 - val_loss: 1.5202\n",
      "Epoch 23/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 0.9235 - val_loss: 1.6012\n",
      "Epoch 24/1000\n",
      "572/572 [==============================] - 0s 500us/step - loss: 0.9307 - val_loss: 1.5971\n",
      "Epoch 25/1000\n",
      "572/572 [==============================] - 0s 490us/step - loss: 0.9222 - val_loss: 1.4975\n",
      "Epoch 26/1000\n",
      "572/572 [==============================] - 0s 493us/step - loss: 0.9172 - val_loss: 1.4744\n",
      "Epoch 27/1000\n",
      "572/572 [==============================] - 0s 499us/step - loss: 0.9076 - val_loss: 1.5687\n",
      "Epoch 28/1000\n",
      "572/572 [==============================] - 0s 492us/step - loss: 0.9019 - val_loss: 1.5989\n",
      "Epoch 29/1000\n",
      "572/572 [==============================] - 0s 491us/step - loss: 0.9230 - val_loss: 1.5563\n",
      "Epoch 30/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.9131 - val_loss: 1.6100\n",
      "Epoch 31/1000\n",
      "572/572 [==============================] - 0s 491us/step - loss: 0.8992 - val_loss: 1.5741\n",
      "Epoch 32/1000\n",
      "572/572 [==============================] - 0s 493us/step - loss: 0.9109 - val_loss: 1.5277\n",
      "Epoch 33/1000\n",
      "572/572 [==============================] - 0s 493us/step - loss: 0.9007 - val_loss: 1.5254\n",
      "Epoch 34/1000\n",
      "572/572 [==============================] - 0s 495us/step - loss: 0.9167 - val_loss: 1.4874\n",
      "Epoch 35/1000\n",
      "572/572 [==============================] - 0s 495us/step - loss: 0.9094 - val_loss: 1.6033\n",
      "Epoch 36/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 0.8889 - val_loss: 1.5281\n",
      "Epoch 37/1000\n",
      "572/572 [==============================] - 0s 494us/step - loss: 0.9061 - val_loss: 1.5951\n",
      "Epoch 38/1000\n",
      "572/572 [==============================] - 0s 493us/step - loss: 0.9099 - val_loss: 1.5066\n",
      "Epoch 39/1000\n",
      "572/572 [==============================] - 0s 500us/step - loss: 0.8929 - val_loss: 1.5673\n",
      "Epoch 40/1000\n",
      "572/572 [==============================] - 0s 493us/step - loss: 0.9019 - val_loss: 1.5308\n",
      "Epoch 41/1000\n",
      "572/572 [==============================] - 0s 494us/step - loss: 0.8763 - val_loss: 1.5512\n",
      "Epoch 42/1000\n",
      "572/572 [==============================] - 0s 496us/step - loss: 0.8959 - val_loss: 1.6908\n",
      "Epoch 43/1000\n",
      "572/572 [==============================] - 0s 500us/step - loss: 0.9021 - val_loss: 1.5255\n",
      "Epoch 44/1000\n",
      "572/572 [==============================] - 0s 490us/step - loss: 0.8711 - val_loss: 1.5086\n",
      "Epoch 45/1000\n",
      "572/572 [==============================] - 0s 499us/step - loss: 0.8884 - val_loss: 1.5484\n",
      "Epoch 46/1000\n",
      "572/572 [==============================] - 0s 493us/step - loss: 0.9091 - val_loss: 1.5129\n",
      "Epoch 47/1000\n",
      "572/572 [==============================] - 0s 496us/step - loss: 0.8777 - val_loss: 1.4731\n",
      "Epoch 48/1000\n",
      "572/572 [==============================] - 0s 496us/step - loss: 0.8809 - val_loss: 1.6043\n",
      "Epoch 49/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8749 - val_loss: 1.5072\n",
      "Epoch 50/1000\n",
      "572/572 [==============================] - 0s 493us/step - loss: 0.8878 - val_loss: 1.4485\n",
      "Epoch 51/1000\n",
      "572/572 [==============================] - 0s 493us/step - loss: 0.8780 - val_loss: 1.4767\n",
      "Epoch 52/1000\n",
      "572/572 [==============================] - 0s 495us/step - loss: 0.8634 - val_loss: 1.5510\n",
      "Epoch 53/1000\n",
      "572/572 [==============================] - 0s 500us/step - loss: 0.8743 - val_loss: 1.5112\n",
      "Epoch 54/1000\n",
      "572/572 [==============================] - 0s 493us/step - loss: 0.8655 - val_loss: 1.4875\n",
      "Epoch 55/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 0.8585 - val_loss: 1.5050\n",
      "Epoch 56/1000\n",
      "572/572 [==============================] - 0s 490us/step - loss: 0.8742 - val_loss: 1.4585\n",
      "Epoch 57/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 0.8649 - val_loss: 1.6069\n",
      "Epoch 58/1000\n",
      "572/572 [==============================] - 0s 495us/step - loss: 0.8583 - val_loss: 1.4563\n",
      "Epoch 59/1000\n",
      "572/572 [==============================] - 0s 491us/step - loss: 0.8449 - val_loss: 1.5255\n",
      "Epoch 60/1000\n",
      "572/572 [==============================] - 0s 494us/step - loss: 0.8741 - val_loss: 1.4320\n",
      "Epoch 61/1000\n",
      "572/572 [==============================] - 0s 495us/step - loss: 0.8722 - val_loss: 1.4483\n",
      "Epoch 62/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8602 - val_loss: 1.4715\n",
      "Epoch 63/1000\n",
      "572/572 [==============================] - 0s 501us/step - loss: 0.8698 - val_loss: 1.5269\n",
      "Epoch 64/1000\n",
      "572/572 [==============================] - 0s 499us/step - loss: 0.8573 - val_loss: 1.4596\n",
      "Epoch 65/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8692 - val_loss: 1.6225\n",
      "Epoch 66/1000\n",
      "572/572 [==============================] - 0s 496us/step - loss: 0.8705 - val_loss: 1.4866\n",
      "Epoch 67/1000\n",
      "572/572 [==============================] - 0s 499us/step - loss: 0.8739 - val_loss: 1.4650\n",
      "Epoch 68/1000\n",
      "572/572 [==============================] - 0s 496us/step - loss: 0.8705 - val_loss: 1.4546\n",
      "Epoch 69/1000\n",
      "572/572 [==============================] - 0s 499us/step - loss: 0.8577 - val_loss: 1.4606\n",
      "Epoch 70/1000\n",
      "572/572 [==============================] - 0s 491us/step - loss: 0.8642 - val_loss: 1.3983\n",
      "Epoch 71/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8585 - val_loss: 1.4595\n",
      "Epoch 72/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 0.8539 - val_loss: 1.4912\n",
      "Epoch 73/1000\n",
      "572/572 [==============================] - 0s 493us/step - loss: 0.8507 - val_loss: 1.4516\n",
      "Epoch 74/1000\n",
      "572/572 [==============================] - 0s 494us/step - loss: 0.8528 - val_loss: 1.5302\n",
      "Epoch 75/1000\n",
      "572/572 [==============================] - 0s 494us/step - loss: 0.8512 - val_loss: 1.4858\n",
      "Epoch 76/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8395 - val_loss: 1.4769\n",
      "Epoch 77/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8633 - val_loss: 1.5758\n",
      "Epoch 78/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572/572 [==============================] - 0s 500us/step - loss: 0.8535 - val_loss: 1.4819\n",
      "Epoch 79/1000\n",
      "572/572 [==============================] - 0s 491us/step - loss: 0.8553 - val_loss: 1.5516\n",
      "Epoch 80/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 0.8527 - val_loss: 1.4240\n",
      "Epoch 81/1000\n",
      "572/572 [==============================] - 0s 490us/step - loss: 0.8565 - val_loss: 1.4580\n",
      "Epoch 82/1000\n",
      "572/572 [==============================] - 0s 494us/step - loss: 0.8588 - val_loss: 1.5787\n",
      "Epoch 83/1000\n",
      "572/572 [==============================] - 0s 496us/step - loss: 0.8692 - val_loss: 1.5003\n",
      "Epoch 84/1000\n",
      "572/572 [==============================] - 0s 492us/step - loss: 0.8609 - val_loss: 1.4510\n",
      "Epoch 85/1000\n",
      "572/572 [==============================] - 0s 502us/step - loss: 0.8583 - val_loss: 1.4459\n",
      "Epoch 86/1000\n",
      "572/572 [==============================] - 0s 494us/step - loss: 0.8562 - val_loss: 1.5574\n",
      "Epoch 87/1000\n",
      "572/572 [==============================] - 0s 490us/step - loss: 0.8499 - val_loss: 1.4168\n",
      "Epoch 88/1000\n",
      "572/572 [==============================] - 0s 496us/step - loss: 0.8504 - val_loss: 1.4915\n",
      "Epoch 89/1000\n",
      "572/572 [==============================] - 0s 490us/step - loss: 0.8548 - val_loss: 1.4924\n",
      "Epoch 90/1000\n",
      "572/572 [==============================] - 0s 494us/step - loss: 0.8404 - val_loss: 1.4570\n",
      "Epoch 91/1000\n",
      "572/572 [==============================] - 0s 503us/step - loss: 0.8645 - val_loss: 1.4916\n",
      "Epoch 92/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8615 - val_loss: 1.4378\n",
      "Epoch 93/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8413 - val_loss: 1.4374\n",
      "Epoch 94/1000\n",
      "572/572 [==============================] - 0s 502us/step - loss: 0.8499 - val_loss: 1.4557\n",
      "Epoch 95/1000\n",
      "572/572 [==============================] - 0s 494us/step - loss: 0.8382 - val_loss: 1.4584\n",
      "Epoch 96/1000\n",
      "572/572 [==============================] - 0s 494us/step - loss: 0.8429 - val_loss: 1.4891\n",
      "Epoch 97/1000\n",
      "572/572 [==============================] - 0s 496us/step - loss: 0.8486 - val_loss: 1.4839\n",
      "Epoch 98/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8507 - val_loss: 1.4666\n",
      "Epoch 99/1000\n",
      "572/572 [==============================] - 0s 502us/step - loss: 0.8343 - val_loss: 1.4051\n",
      "Epoch 100/1000\n",
      "572/572 [==============================] - 0s 493us/step - loss: 0.8402 - val_loss: 1.4502\n",
      "Epoch 101/1000\n",
      "572/572 [==============================] - 0s 491us/step - loss: 0.8535 - val_loss: 1.4681\n",
      "Epoch 102/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 0.8393 - val_loss: 1.4181\n",
      "Epoch 103/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8537 - val_loss: 1.4598\n",
      "Epoch 104/1000\n",
      "572/572 [==============================] - 0s 495us/step - loss: 0.8446 - val_loss: 1.4450\n",
      "Epoch 105/1000\n",
      "572/572 [==============================] - 0s 496us/step - loss: 0.8427 - val_loss: 1.5183\n",
      "Epoch 106/1000\n",
      "572/572 [==============================] - 0s 496us/step - loss: 0.8543 - val_loss: 1.4809\n",
      "Epoch 107/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 0.8355 - val_loss: 1.4459\n",
      "Epoch 108/1000\n",
      "572/572 [==============================] - 0s 503us/step - loss: 0.8519 - val_loss: 1.4545\n",
      "Epoch 109/1000\n",
      "572/572 [==============================] - 0s 489us/step - loss: 0.8458 - val_loss: 1.4806\n",
      "Epoch 110/1000\n",
      "572/572 [==============================] - 0s 502us/step - loss: 0.8535 - val_loss: 1.4556\n",
      "Epoch 111/1000\n",
      "572/572 [==============================] - 0s 503us/step - loss: 0.8430 - val_loss: 1.5216\n",
      "Epoch 112/1000\n",
      "572/572 [==============================] - 0s 502us/step - loss: 0.8617 - val_loss: 1.4978\n",
      "Epoch 113/1000\n",
      "572/572 [==============================] - 0s 499us/step - loss: 0.8635 - val_loss: 1.4672\n",
      "Epoch 114/1000\n",
      "572/572 [==============================] - 0s 495us/step - loss: 0.8323 - val_loss: 1.4336\n",
      "Epoch 115/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8394 - val_loss: 1.4844\n",
      "Epoch 116/1000\n",
      "572/572 [==============================] - 0s 495us/step - loss: 0.8604 - val_loss: 1.4304\n",
      "Epoch 117/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8398 - val_loss: 1.4529\n",
      "Epoch 118/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 0.8368 - val_loss: 1.4353\n",
      "Epoch 119/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8497 - val_loss: 1.4939\n",
      "Epoch 120/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 0.8432 - val_loss: 1.5171\n",
      "Epoch 121/1000\n",
      "572/572 [==============================] - 0s 511us/step - loss: 0.8522 - val_loss: 1.4786\n",
      "Epoch 122/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8425 - val_loss: 1.4949\n",
      "Epoch 123/1000\n",
      "572/572 [==============================] - 0s 501us/step - loss: 0.8392 - val_loss: 1.4168\n",
      "Epoch 124/1000\n",
      "572/572 [==============================] - 0s 494us/step - loss: 0.8482 - val_loss: 1.4426\n",
      "Epoch 125/1000\n",
      "572/572 [==============================] - 0s 494us/step - loss: 0.8378 - val_loss: 1.4349\n",
      "Epoch 126/1000\n",
      "572/572 [==============================] - 0s 501us/step - loss: 0.8394 - val_loss: 1.4449\n",
      "Epoch 127/1000\n",
      "572/572 [==============================] - 0s 499us/step - loss: 0.8636 - val_loss: 1.4277\n",
      "Epoch 128/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 0.8761 - val_loss: 1.4510\n",
      "Epoch 129/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 0.8399 - val_loss: 1.5429\n",
      "Epoch 130/1000\n",
      "572/572 [==============================] - 0s 495us/step - loss: 0.8264 - val_loss: 1.4943\n",
      "Epoch 131/1000\n",
      "572/572 [==============================] - 0s 495us/step - loss: 0.8437 - val_loss: 1.4587\n",
      "Epoch 132/1000\n",
      "572/572 [==============================] - 0s 499us/step - loss: 0.8416 - val_loss: 1.5497\n",
      "Epoch 133/1000\n",
      "572/572 [==============================] - 0s 508us/step - loss: 0.8368 - val_loss: 1.4438\n",
      "Epoch 134/1000\n",
      "572/572 [==============================] - 0s 496us/step - loss: 0.8458 - val_loss: 1.4558\n",
      "Epoch 135/1000\n",
      "572/572 [==============================] - 0s 490us/step - loss: 0.8482 - val_loss: 1.5192\n",
      "Epoch 136/1000\n",
      "572/572 [==============================] - 0s 500us/step - loss: 0.8630 - val_loss: 1.4527\n",
      "Epoch 137/1000\n",
      "572/572 [==============================] - 0s 500us/step - loss: 0.8561 - val_loss: 1.4145\n",
      "Epoch 138/1000\n",
      "572/572 [==============================] - 0s 503us/step - loss: 0.8538 - val_loss: 1.4059\n",
      "Epoch 139/1000\n",
      "572/572 [==============================] - 0s 505us/step - loss: 0.8401 - val_loss: 1.4822\n",
      "Epoch 140/1000\n",
      "572/572 [==============================] - 0s 502us/step - loss: 0.8484 - val_loss: 1.4571\n",
      "Epoch 141/1000\n",
      "572/572 [==============================] - 0s 491us/step - loss: 0.8410 - val_loss: 1.3991\n",
      "Epoch 142/1000\n",
      "572/572 [==============================] - 0s 505us/step - loss: 0.8431 - val_loss: 1.4738\n",
      "Epoch 143/1000\n",
      "572/572 [==============================] - 0s 489us/step - loss: 0.8454 - val_loss: 1.4105\n",
      "Epoch 144/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 0.8408 - val_loss: 1.4903\n",
      "Epoch 145/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 0.8439 - val_loss: 1.4778\n",
      "Epoch 146/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8532 - val_loss: 1.4501\n",
      "Epoch 147/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 0.8442 - val_loss: 1.4556\n",
      "Epoch 148/1000\n",
      "572/572 [==============================] - 0s 505us/step - loss: 0.8451 - val_loss: 1.4957\n",
      "Epoch 149/1000\n",
      "572/572 [==============================] - 0s 505us/step - loss: 0.8436 - val_loss: 1.5198\n",
      "Epoch 150/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8463 - val_loss: 1.5904\n",
      "Epoch 151/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8473 - val_loss: 1.5564\n",
      "Epoch 152/1000\n",
      "572/572 [==============================] - 0s 493us/step - loss: 0.8456 - val_loss: 1.4563\n",
      "Epoch 153/1000\n",
      "572/572 [==============================] - 0s 489us/step - loss: 0.8452 - val_loss: 1.5282\n",
      "Epoch 154/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8403 - val_loss: 1.5076\n",
      "Epoch 155/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572/572 [==============================] - 0s 491us/step - loss: 0.8443 - val_loss: 1.4331\n",
      "Epoch 156/1000\n",
      "572/572 [==============================] - 0s 503us/step - loss: 0.8537 - val_loss: 1.4532\n",
      "Epoch 157/1000\n",
      "572/572 [==============================] - 0s 500us/step - loss: 0.8442 - val_loss: 1.4536\n",
      "Epoch 158/1000\n",
      "572/572 [==============================] - 0s 494us/step - loss: 0.8492 - val_loss: 1.4044\n",
      "Epoch 159/1000\n",
      "572/572 [==============================] - 0s 495us/step - loss: 0.8481 - val_loss: 1.4332\n",
      "Epoch 160/1000\n",
      "572/572 [==============================] - 0s 499us/step - loss: 0.8308 - val_loss: 1.4649\n",
      "Epoch 161/1000\n",
      "572/572 [==============================] - 0s 495us/step - loss: 0.8530 - val_loss: 1.5590\n",
      "Epoch 162/1000\n",
      "572/572 [==============================] - 0s 495us/step - loss: 0.8493 - val_loss: 1.4456\n",
      "Epoch 163/1000\n",
      "572/572 [==============================] - 0s 496us/step - loss: 0.8376 - val_loss: 1.4319\n",
      "Epoch 164/1000\n",
      "572/572 [==============================] - 0s 498us/step - loss: 0.8550 - val_loss: 1.4098\n",
      "Epoch 165/1000\n",
      "572/572 [==============================] - 0s 501us/step - loss: 0.8555 - val_loss: 1.4210\n",
      "Epoch 166/1000\n",
      "572/572 [==============================] - 0s 500us/step - loss: 0.8356 - val_loss: 1.5298\n",
      "Epoch 167/1000\n",
      "572/572 [==============================] - 0s 499us/step - loss: 0.8501 - val_loss: 1.4627\n",
      "Epoch 168/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 0.8769 - val_loss: 1.5650\n",
      "Epoch 169/1000\n",
      "572/572 [==============================] - 0s 499us/step - loss: 0.8423 - val_loss: 1.4618\n",
      "Epoch 170/1000\n",
      "572/572 [==============================] - 0s 497us/step - loss: 0.8276 - val_loss: 1.5699\n"
     ]
    }
   ],
   "source": [
    "callbacks=[rnaNet.lrate, cb.EarlyStopping(monitor='val_loss', patience=100, mode='auto')]\n",
    "rnaNet.train(epochs=1000, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import scatter_plot, heatmap\n",
    "rnaNet.predict()\n",
    "rnaNet.pca()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scatter_plot(rnaNet.source_pca_df, rnaNet.target_pca_df, title=\"before\")\n",
    "scatter_plot(rnaNet.calibrated_source_pca_df, rnaNet.target_pca_df, title=\"after\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "heatmap(rnaNet.source_df, rnaNet.target_df, title=\"before\")\n",
    "heatmap(rnaNet.calibrated_source_df, rnaNet.target_df, title=\"after\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CostFunctions as cf\n",
    "from keras import backend as K\n",
    "\n",
    "source = rnaNet.source.astype('float32')\n",
    "target = rnaNet.target.astype('float32')\n",
    "calibrated_source = rnaNet.calibrated_source.astype('float32')\n",
    "\n",
    "mmd = cf.MMD(source, target, MMDTargetSampleSize=target.shape[0], n_neighbors=10)\n",
    "mmd_before = K.eval(mmd.cost(source, target))\n",
    "mmd_after = K.eval(mmd.cost(calibrated_source, target))\n",
    "\n",
    "print(\"MMD before: %0.10f\" % mmd_before)\n",
    "print(\"MMD after: %0.10f\" % mmd_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = 'calibrated-unnorm-norm-20PC-GTEX-breast-prostate-thyroid.csv'\n",
    "save_path = os.path.join(io.DeepLearningRoot(), 'data/unnorm/breast-prostate-thyroid/' + save_file)\n",
    "\n",
    "rnaNet.save_calibrated(path=save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
